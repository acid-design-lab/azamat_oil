{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, LSTM, TimeDistributed, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, TimeDistributed, LSTM, Dense, Concatenate, RepeatVector, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your directory where files are located \n",
    "# tensors loading\n",
    "\n",
    "files_dir = '../data/processed/'\n",
    "loaded_tensors = dict()\n",
    "for file in [x for x in os.listdir(files_dir) if x.endswith(\"tfrecord\")]:\n",
    "    serialized_tensor = tf.io.read_file(os.path.join(files_dir,file))\n",
    "    loaded_tensors[file] = tf.io.parse_tensor(serialized_tensor, out_type=tf.float32)\n",
    "\n",
    "# coordinates loading \n",
    "df_coords = pd.read_excel(os.path.join(files_dir,\"Coordinates_processed.xlsx\"),sheet_name=['X','Y','TVD1','TVD2'],header=None)\n",
    "df_X_Y = pd.DataFrame({col: list(zip(df_coords[\"X\"][col], df_coords[\"Y\"][col])) for col in  df_coords[\"X\"].columns})\n",
    "df_TVD = pd.DataFrame({col: list(zip(df_coords[\"TVD1\"][col], df_coords[\"TVD2\"][col])) for col in  df_coords[\"TVD1\"].columns})\n",
    "\n",
    "# time loading \n",
    "time_sequence = pd.read_csv(os.path.join(files_dir,\"Dates_processed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление приемистости "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '../data/'\n",
    "df_injectivity = pd.read_excel(os.path.join(raw_data_dir,\"raw_data.xlsx\"),sheet_name=['приемистость'])\n",
    "df_injectivity = df_injectivity[\"приемистость\"].drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример кода преобразования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial matrix values\n",
    "matrix_values = [\n",
    "    ['p22', 'p23', 'p24', 'p30', 'p31', 'p32'],\n",
    "    ['p20', 'INJE3', 'p21', 'p28', 'INJE4', 'p29'],\n",
    "    ['p17', 'p18', 'p19', 'p25', 'p26', 'p27'],\n",
    "    ['p6', 'p7', 'p8', 'p14', 'p15', 'p16'],\n",
    "    ['p4', 'INJE1', 'p5', 'p12', 'INJE2', 'p13'],\n",
    "    ['p1', 'p2', 'p3', 'p9', 'p10', 'p11']\n",
    "]\n",
    "\n",
    "# Convert to numpy array and repeat\n",
    "matrix_array = np.array(matrix_values)\n",
    "repeated_matrix = np.tile(matrix_array, (121, 1, 1))\n",
    "\n",
    "# Create a mask to remove the specific elements\n",
    "mask = np.ones(repeated_matrix.shape, dtype=bool)\n",
    "mask[:, 1, 1] = False\n",
    "mask[:, 1, 4] = False\n",
    "mask[:, 4, 1] = False\n",
    "mask[:, 4, 4] = False  # This seems to be a repeat; ensure this element is removed\n",
    "\n",
    "# Apply the mask and reshape the array\n",
    "filtered_array = repeated_matrix[mask].reshape(121, 4, 8)\n",
    "\n",
    "# Display the shape to verify\n",
    "print(filtered_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['p22', 'p23', 'p24', 'p30', 'p31', 'p32', 'p20', 'p21'],\n",
       "       ['p28', 'p29', 'p17', 'p18', 'p19', 'p25', 'p26', 'p27'],\n",
       "       ['p6', 'p7', 'p8', 'p14', 'p15', 'p16', 'p4', 'p5'],\n",
       "       ['p12', 'p13', 'p1', 'p2', 'p3', 'p9', 'p10', 'p11']], dtype='<U5')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование массивов (121,6,6) в (121,4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_data = loaded_tensors['debit_neft_series.tfrecord'].numpy()\n",
    "water_data = loaded_tensors['debit_water_series.tfrecord'].numpy()\n",
    "reservoir_pressure_data = loaded_tensors['debit_plast_pressure_series.tfrecord'].numpy()\n",
    "bottomhole_pressure_data = loaded_tensors['debit_zaboy_pressure_series.tfrecord'].numpy()\n",
    "\n",
    "# Create a mask to remove the specific elements\n",
    "mask = np.ones(oil_data.shape, dtype=bool)\n",
    "mask[:, 1, 1] = False\n",
    "mask[:, 1, 4] = False\n",
    "mask[:, 4, 1] = False\n",
    "mask[:, 4, 4] = False  # This seems to be a repeat; ensure this element is removed\n",
    "\n",
    "# Apply the mask and reshape the array\n",
    "oil_data = oil_data[mask].reshape(121, 4, 8)\n",
    "water_data = water_data[mask].reshape(121, 4, 8)\n",
    "reservoir_pressure_data = reservoir_pressure_data[mask].reshape(121, 4, 8)\n",
    "bottomhole_pressure_data = bottomhole_pressure_data[mask].reshape(121, 4, 8)\n",
    "\n",
    "# reshape coords and TVD\n",
    "df_X_Y = df_X_Y.values[mask[0]].reshape(4, 8)\n",
    "df_TVD = df_TVD.values[mask[0]].reshape(4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, TimeDistributed, LSTM, Dense, Concatenate, RepeatVector, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Parameters\n",
    "total_time_steps = 121\n",
    "train_time_steps = 84\n",
    "test_time_steps = total_time_steps - train_time_steps\n",
    "map_height = 6\n",
    "map_width = 6\n",
    "batch_size = 1\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Function to create CNN layers for input tensors\n",
    "def create_cnn(input_tensor):\n",
    "    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(input_tensor)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(x)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    return x\n",
    "\n",
    "# Input tensors for training model\n",
    "inputs_train = {\n",
    "    'oil': Input(shape=(train_time_steps, map_height, map_width, 1), name='input_oil'),\n",
    "    'water': Input(shape=(train_time_steps, map_height, map_width, 1), name='input_water'),\n",
    "    'reservoir_pressure': Input(shape=(train_time_steps, map_height, map_width, 1), name='input_reservoir_pressure'),\n",
    "    'bottomhole_pressure': Input(shape=(train_time_steps, map_height, map_width, 1), name='input_bottomhole_pressure'),\n",
    "    'coordinates': Input(shape=(map_height, map_width, 2), name='input_coordinates'),\n",
    "    'tvd': Input(shape=(map_height, map_width, 2), name='input_tvd'),\n",
    "    'injectivity': Input(shape=(train_time_steps, 4), name='input_injectivity')  # добавляем приемистость\n",
    "}\n",
    "\n",
    "# Input tensors for testing model\n",
    "inputs_test = {\n",
    "    'oil': Input(shape=(test_time_steps, map_height, map_width, 1), name='input_oil'),\n",
    "    'water': Input(shape=(test_time_steps, map_height, map_width, 1), name='input_water'),\n",
    "    'reservoir_pressure': Input(shape=(test_time_steps, map_height, map_width, 1), name='input_reservoir_pressure'),\n",
    "    'bottomhole_pressure': Input(shape=(test_time_steps, map_height, map_width, 1), name='input_bottomhole_pressure'),\n",
    "    'coordinates': Input(shape=(map_height, map_width, 2), name='input_coordinates'),\n",
    "    'tvd': Input(shape=(map_height, map_width, 2), name='input_tvd'),\n",
    "    'injectivity': Input(shape=(test_time_steps, 4), name='input_injectivity')  # добавляем приемистость\n",
    "}\n",
    "\n",
    "# Creating CNN layers for training and testing\n",
    "cnn_train = {name: create_cnn(tensor) for name, tensor in inputs_train.items() if name not in ['coordinates', 'tvd', 'injectivity']}\n",
    "cnn_test = {name: create_cnn(tensor) for name, tensor in inputs_test.items() if name not in ['coordinates', 'tvd', 'injectivity']}\n",
    "\n",
    "# Flatten and repeat coordinates and TVD for training\n",
    "flattened_coordinates_train = Flatten()(inputs_train['coordinates'])\n",
    "repeated_coordinates_train = RepeatVector(train_time_steps)(flattened_coordinates_train)\n",
    "flattened_tvd_train = Flatten()(inputs_train['tvd'])\n",
    "repeated_tvd_train = RepeatVector(train_time_steps)(flattened_tvd_train)\n",
    "\n",
    "# Flatten and repeat coordinates and TVD for testing\n",
    "flattened_coordinates_test = Flatten()(inputs_test['coordinates'])\n",
    "repeated_coordinates_test = RepeatVector(test_time_steps)(flattened_coordinates_test)\n",
    "flattened_tvd_test = Flatten()(inputs_test['tvd'])\n",
    "repeated_tvd_test = RepeatVector(test_time_steps)(flattened_tvd_test)\n",
    "\n",
    "# Concatenating all inputs for training, including injectivity\n",
    "merged_train = Concatenate(axis=-1)(list(cnn_train.values()))\n",
    "flattened_merged_train = TimeDistributed(Flatten())(merged_train)\n",
    "combined_train = Concatenate(axis=-1)([flattened_merged_train, repeated_coordinates_train, repeated_tvd_train, inputs_train['injectivity']])\n",
    "# Example of adding more layers to the LSTM model\n",
    "lstm_out_train = LSTM(128, return_sequences=True)(combined_train)\n",
    "lstm_out_train = LSTM(64, return_sequences=False)(lstm_out_train)\n",
    "dropout_train = Dropout(dropout_rate)(lstm_out_train)\n",
    "dense_1_train = Dense(128, activation='relu')(dropout_train)\n",
    "dropout_train_2 = Dropout(dropout_rate)(dense_1_train)\n",
    "dense_2_train = Dense(72, activation='relu')(dropout_train_2)\n",
    "output_train = Dense(map_height * map_width, activation='linear', name='output')(dense_2_train)\n",
    "output_train = Reshape((map_height, map_width))(output_train)\n",
    "\n",
    "# Training model\n",
    "model_train = Model(inputs=list(inputs_train.values()), outputs=output_train)\n",
    "model_train.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Concatenating all inputs for testing, including injectivity\n",
    "merged_test = Concatenate(axis=-1)(list(cnn_test.values()))\n",
    "flattened_merged_test = TimeDistributed(Flatten())(merged_test)\n",
    "combined_test = Concatenate(axis=-1)([flattened_merged_test, repeated_coordinates_test, repeated_tvd_test, inputs_test['injectivity']])\n",
    "lstm_out_test = LSTM(128, return_sequences=False)(combined_test)\n",
    "dropout_test = Dropout(dropout_rate)(lstm_out_test)\n",
    "dense_1_test = Dense(128, activation='relu')(dropout_test)\n",
    "dropout_test_2 = Dropout(dropout_rate)(dense_1_test)\n",
    "dense_2_test = Dense(72, activation='relu')(dropout_test_2)\n",
    "output_test = Dense(map_height * map_width, activation='linear', name='output')(dense_2_test)\n",
    "output_test = Reshape((map_height, map_width))(output_test)\n",
    "\n",
    "# Testing model\n",
    "model_test = Model(inputs=list(inputs_test.values()), outputs=output_test)\n",
    "model_test.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Load and preprocess data (Assuming loaded_tensors and injectivity_array are already defined)\n",
    "oil_data = np.expand_dims(loaded_tensors['debit_neft_series.tfrecord'].numpy(), axis=-1)\n",
    "water_data = np.expand_dims(loaded_tensors['debit_water_series.tfrecord'].numpy(), axis=-1)\n",
    "reservoir_pressure_data = np.expand_dims(loaded_tensors['debit_plast_pressure_series.tfrecord'].numpy(), axis=-1)\n",
    "bottomhole_pressure_data = np.expand_dims(loaded_tensors['debit_zaboy_pressure_series.tfrecord'].numpy(), axis=-1)\n",
    "\n",
    "# Transforming df_X_Y and df_TVD to numpy arrays of required shape\n",
    "coordinates_data = np.array(df_X_Y.applymap(lambda x: list(x)).values.tolist()).reshape((batch_size, map_height, map_width, 2))\n",
    "tvd_data = np.array(df_TVD.applymap(lambda x: list(x)).values.tolist()).reshape((batch_size, map_height, map_width, 2))\n",
    "\n",
    "# Ensuring data shapes match expected shape\n",
    "oil_data = oil_data.reshape((batch_size, total_time_steps, map_height, map_width, 1))\n",
    "water_data = water_data.reshape((batch_size, total_time_steps, map_height, map_width, 1))\n",
    "reservoir_pressure_data = reservoir_pressure_data.reshape((batch_size, total_time_steps, map_height, map_width, 1))\n",
    "bottomhole_pressure_data = bottomhole_pressure_data.reshape((batch_size, total_time_steps, map_height, map_width, 1))\n",
    "\n",
    "# Проверка на наличие пропусков и заполнение нулями или средними значениями\n",
    "oil_data = np.nan_to_num(oil_data)\n",
    "water_data = np.nan_to_num(water_data)\n",
    "reservoir_pressure_data = np.nan_to_num(reservoir_pressure_data)\n",
    "bottomhole_pressure_data = np.nan_to_num(bottomhole_pressure_data)\n",
    "injectivity_data = np.nan_to_num(injectivity_array.reshape((batch_size, total_time_steps, 4)))\n",
    "\n",
    "# Масштабирование данных\n",
    "scalers = {}\n",
    "for data_name, data in zip(['oil', 'water', 'reservoir_pressure', 'bottomhole_pressure', 'injectivity'],\n",
    "                           [oil_data, water_data, reservoir_pressure_data, bottomhole_pressure_data, injectivity_data]):\n",
    "    scalers[data_name] = MinMaxScaler()\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    scaled_data = scalers[data_name].fit_transform(data_reshaped)\n",
    "    data[:] = scaled_data.reshape(data.shape)\n",
    "\n",
    "# Splitting data into training and test sets\n",
    "oil_train, oil_test = oil_data[:, :train_time_steps], oil_data[:, train_time_steps:]\n",
    "water_train, water_test = water_data[:, :train_time_steps], water_data[:, train_time_steps:]\n",
    "reservoir_pressure_train, reservoir_pressure_test = reservoir_pressure_data[:, :train_time_steps], reservoir_pressure_data[:, train_time_steps:]\n",
    "bottomhole_pressure_train, bottomhole_pressure_test = bottomhole_pressure_data[:, :train_time_steps], bottomhole_pressure_data[:, train_time_steps:]\n",
    "\n",
    "# Splitting injectivity data into training and test sets\n",
    "injectivity_train, injectivity_test = injectivity_data[:, :train_time_steps], injectivity_data[:, train_time_steps:]\n",
    "\n",
    "# Define output for training and testing\n",
    "output_train_data = oil_data[:, :train_time_steps, :, :, 0].reshape((batch_size, train_time_steps, map_height, map_width))\n",
    "output_test_data = oil_data[:, train_time_steps:, :, :, 0].reshape((batch_size, test_time_steps, map_height, map_width))\n",
    "\n",
    "# Adding EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Training the model with validation\n",
    "history = model_train.fit(\n",
    "    [oil_train, water_train, reservoir_pressure_train, bottomhole_pressure_train, coordinates_data, tvd_data, injectivity_train],\n",
    "    output_train_data,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
